reset에서 디렉 델타는 무한대라서 헤비사이드 함수가 단순화 해준다.
feed-foward 시냅스 가중치 행렬은 다음 레이어, recurrent 시냅스 가중치 행렬은 현재 레이어를 의미한다.
R 저항: 값이 클 수록 막전위가 빠르게 변화한다.
tau 막시간 상수가 크면 막전위가 빠르게 증가한다.

코드 세미나 할 때, 코드와 수식을 비교해보자.
신경망 그림을 같이 보자.

==========================

미분 방정식: i - rest
미분 방정식을 푸는 것도 공부를 해야된다.
전압의 변화량은 입력 전위에 의해서 좌우가 된다.
전류가 없으면, 지수적으로 감소. 있으면 증가한다.
t 가 아니라 tau가 크면 빠르게 증가한다.
R: 값이 클 수록 막전위가 빠르게 변화한다.
Tmem -> RC
그래프 그려서 직접 확인하기

reset 스파이크 합에 의해서 입력 뉴런의 전류가 결정된다. 그걸 수식으로 표현한 것
스파이크 동시에 발생하지 않는다.
스파이크의 합이 다음 뉴런에 전달되는 것을 설명

(0), (1)이 있는 input 수식을 수정해보자.
(0)은 레이어를 의미하는 것이다.
RNN
input 함수를 RNN에 맞게 더 정리해야될듯하다.
feed-foward 시냅스 가중치 행렬은 다음 레이어, recurrent 시냅스 가중치 행렬은 현재 레이어를 의미한다.
e^-l 패턴

붕괘 -> 붕괴

reset에서 디렉 델타는 무한대라서 헤비사이드 함수가 단순화 해준다.

감소가 e^-1 패턴이다.

코드를 보고 프로그램을 해석

코드 세미나 할 때, 코드와 수식을 비교해보자.
신경망 그림을 같이 보자.
